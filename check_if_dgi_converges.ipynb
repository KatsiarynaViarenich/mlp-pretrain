{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def reset_dataset_dependent_parameters(args):\n",
    "    if args.dataset == \"Flickr\":\n",
    "        args.num_classes = 7\n",
    "        args.num_feats = 500\n",
    "\n",
    "    elif args.dataset == \"Reddit\":\n",
    "        args.num_classes = 41\n",
    "        args.num_feats = 602\n",
    "\n",
    "    elif args.dataset == \"Reddit2\":\n",
    "        args.num_classes = 41\n",
    "        args.num_feats = 602\n",
    "\n",
    "    elif args.dataset == \"ogbn-products\":\n",
    "        args.multi_label = False\n",
    "        args.num_classes = 47\n",
    "        args.num_feats = 100\n",
    "\n",
    "    elif args.dataset == \"AmazonProducts\":\n",
    "        args.multi_label = True\n",
    "        args.num_classes = 107\n",
    "        args.num_feats = 200\n",
    "\n",
    "    elif args.dataset == \"Yelp\":\n",
    "        args.multi_label = True\n",
    "        args.num_classes = 100\n",
    "        args.num_feats = 300\n",
    "\n",
    "    elif args.dataset == \"ogbn-arxiv\":\n",
    "        args.num_feats = 128\n",
    "        args.num_classes = 40\n",
    "        args.N_nodes = 169343\n",
    "    return args"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.\n",
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mkats\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.15.1 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.15.0"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>./wandb/wandb/run-20230503_162621-n6g8sb9o</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/kats/pretrain-mlpinit/runs/n6g8sb9o' target=\"_blank\">flowing-dragon-42</a></strong> to <a href='https://wandb.ai/kats/pretrain-mlpinit' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/kats/pretrain-mlpinit' target=\"_blank\">https://wandb.ai/kats/pretrain-mlpinit</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/kats/pretrain-mlpinit/runs/n6g8sb9o' target=\"_blank\">https://wandb.ai/kats/pretrain-mlpinit/runs/n6g8sb9o</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://docs.google.com/uc?export=download&id=1crmsTbd1-2sEXsGwa2IKnIB7Zd3TmUsy&confirm=t\n",
      "Downloading https://docs.google.com/uc?export=download&id=1join-XdvX3anJU_MLVtick7MgeAQiWIZ&confirm=t\n",
      "wandb: Network error (ReadTimeout), entering retry loop.\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "from ogb.nodeproppred import Evaluator, PygNodePropPredDataset\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch_geometric.loader import NeighborSampler\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch.nn import Linear\n",
    "from typing import Tuple, Union\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch_geometric.nn.dense.linear import Linear\n",
    "from torch_geometric.typing import OptPairTensor\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "from torch_geometric.nn.models import DeepGraphInfomax\n",
    "import wandb\n",
    "import argparse\n",
    "\n",
    "import sys\n",
    "from src.load_dataset import load_data\n",
    "\n",
    "\n",
    "num_feats = 128\n",
    "num_classes = 40\n",
    "N_nodes = 169343\n",
    "\n",
    "\n",
    "wandb.init(\n",
    "    project=\"pretrain-mlpinit\",\n",
    "    dir=\"./wandb\",\n",
    "    config={\n",
    "        \"dataset_name\": \"ogbn-arxiv\",\n",
    "        \"batch_size\": 4096,\n",
    "        \"num_layers\": 4,\n",
    "        \"hidden_channels\": 1024,\n",
    "        \"percent_corrupted\": 0.25,\n",
    "        \"custom_step\": 0\n",
    "    }\n",
    ")\n",
    "\n",
    "dataset_dir = \"./data\"\n",
    "num_workers = 4\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "root = osp.join(dataset_dir, wandb.config.dataset_name)\n",
    "\n",
    "(data, x, y, split_masks, evaluator, processed_dir) = load_data(\"Flickr\", \"./data\")\n",
    "\n",
    "\n",
    "train_idx = split_masks[\"train\"]\n",
    "\n",
    "x_train = data.x[split_masks[\"train\"]]\n",
    "y_train = data.y[split_masks[\"train\"]].reshape(-1).type(torch.long)\n",
    "\n",
    "\n",
    "print(\"data.x.shape:\", data.x.shape)\n",
    "print(\"data.y.shape:\", data.y.shape)\n",
    "print(\"data.x.type:\", x.dtype)\n",
    "print(\"data.y.type:\", y.dtype)\n",
    "print(\"x_train.shape:\", x_train.shape)\n",
    "print(\"y_train.shape:\", y_train.shape)\n",
    "\n",
    "y = data.y.squeeze().type(torch.long)\n",
    "\n",
    "x_y_train_mlpinit = data_utils.TensorDataset(x_train, y_train)\n",
    "x_y_all_mlpinit = data_utils.TensorDataset(x, y)\n",
    "\n",
    "train_mlpinit_loader = data_utils.DataLoader(\n",
    "    x_y_train_mlpinit,\n",
    "    batch_size=wandb.config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "all_mlpinit_loader = data_utils.DataLoader(\n",
    "    x_y_all_mlpinit,\n",
    "    batch_size=wandb.config.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "class SAGEConv_PeerMLP(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A PyTorch module implementing a simplified GraphSAGE convolution-like multilayer perceptron (MLP) layer.\n",
    "\n",
    "    This layer performs a linear transformation on the input node features, optionally normalizing\n",
    "    the output and adding a root weight.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels: Union[int, Tuple[int, int]],\n",
    "            out_channels: int,\n",
    "            normalize: bool = False,\n",
    "            root_weight: bool = True,\n",
    "            bias: bool = True,\n",
    "            **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.normalize = normalize\n",
    "        self.root_weight = root_weight\n",
    "\n",
    "        if isinstance(in_channels, int):\n",
    "            in_channels = (in_channels, in_channels)\n",
    "\n",
    "        self.lin_l = Linear(in_channels[0], out_channels, bias=bias)\n",
    "        if self.root_weight:\n",
    "            self.lin_r = Linear(in_channels[1], out_channels, bias=False)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.lin_l.reset_parameters()\n",
    "        if self.root_weight:\n",
    "            self.lin_r.reset_parameters()\n",
    "\n",
    "    def forward(self, x: Union[Tensor, OptPairTensor]) -> Tensor:\n",
    "        \"\"\"\"\"\"\n",
    "        if isinstance(x, Tensor):\n",
    "            x: OptPairTensor = (x, x)\n",
    "\n",
    "        out = x[1]\n",
    "        out = self.lin_l(out)\n",
    "\n",
    "        x_r = x[1]\n",
    "        if self.root_weight and x_r is not None:\n",
    "            out += self.lin_r(x_r)\n",
    "\n",
    "        if self.normalize:\n",
    "            out = F.normalize(out, p=2.0, dim=-1)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(SAGEConv_PeerMLP(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(SAGEConv_PeerMLP(hidden_channels, hidden_channels))\n",
    "        self.convs.append(SAGEConv_PeerMLP(hidden_channels, out_channels))\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        prev_x = x\n",
    "        for i in range(self.num_layers):\n",
    "            x_target = x\n",
    "            x = self.convs[i]((x, x_target))\n",
    "            if i != self.num_layers - 1:\n",
    "                x = F.relu(x)\n",
    "        if x.shape[1] == prev_x.shape[1]:\n",
    "            x = x + prev_x\n",
    "        return x.log_softmax(dim=-1)\n",
    "\n",
    "\n",
    "model_mlpinit = MLP(\n",
    "    in_channels=dataset.num_features,\n",
    "    hidden_channels=wandb.config.hidden_channels,\n",
    "    out_channels=dataset.num_classes,\n",
    "    num_layers=wandb.config.num_layers,\n",
    ")\n",
    "\n",
    "model_mlpinit = model_mlpinit.to(device)\n",
    "optimizer_model_mlpinit = torch.optim.Adam(model_mlpinit.parameters(), lr=0.001, weight_decay=0.0)\n",
    "\n",
    "\n",
    "def train_mlpinit():\n",
    "    def index_corruption(x):\n",
    "        num_nodes = x.size()[0]\n",
    "        mask = torch.ones(num_nodes, num_feats)\n",
    "        mask[:][torch.randperm(num_nodes)[:int(num_feats*wandb.config.percent_corrupted)]] = 0\n",
    "        mask = mask.bool().to(device)\n",
    "\n",
    "        x = torch.where(mask.bool(), x, torch.zeros_like(x))\n",
    "        return x\n",
    "\n",
    "    def dropout_corruption(x, p=0.05):\n",
    "        mask = torch.empty_like(x).bernoulli_(p)\n",
    "        x = torch.where(mask.bool(), x, torch.zeros_like(x))\n",
    "        return x\n",
    "\n",
    "    def summary(z, *args, **kwargs):\n",
    "        return torch.sigmoid(z.mean(dim=0))\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    unsupervised_model = DeepGraphInfomax(hidden_channels=num_classes, encoder=model_mlpinit, summary=summary,\n",
    "                                          corruption=dropout_corruption)\n",
    "    unsupervised_model.to(device)\n",
    "    unsupervised_model.train()\n",
    "    for x, _ in tqdm(train_mlpinit_loader):\n",
    "        x = x.to(device)\n",
    "\n",
    "        optimizer_model_mlpinit.zero_grad()\n",
    "        pos_z, neg_z, summary = unsupervised_model(x)\n",
    "        loss = unsupervised_model.loss(pos_z, neg_z, summary)\n",
    "        loss.backward()\n",
    "        optimizer_model_mlpinit.step()\n",
    "\n",
    "        total_loss += float(loss)\n",
    "\n",
    "    loss_percent = total_loss / len(train_mlpinit_loader)\n",
    "    wandb.log({\"loss_dgi\": loss_percent})\n",
    "    unsupervised_model.eval()\n",
    "    return loss_percent, 0\n",
    "\n",
    "model_mlpinit.reset_parameters()\n",
    "\n",
    "for epoch in range(1, 50):\n",
    "    loss_without_shadow, acc = train_mlpinit()    # p is the probability of dropping a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
